% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Train.R, R/Tune.R
\name{train2MXGb}
\alias{train2MXGb}
\title{Train function for difference score method using XGBoost models}
\usage{
train2MXGb(Y, Trt, X, opts = NULL)

train2MXGb(Y, Trt, X, opts = NULL)
}
\arguments{
\item{Y}{the binary response varibale}

\item{Trt}{the binary treatment variable}

\item{X}{the numeric covariates matrix}

\item{opts}{a list of options over that parameters of generalized linear models is chosen (NOTE: should be only ONE common list of parameters for the two models, see Example)}
}
\value{
a list of models that are able to classify clients into treatment and control group

two lists of parameters found over grid search that then are passed in TrainFunc, one list of parameters for model based on the treated observations (Trt = 1), and one list of parameters for model based on the control observations (Trt = 0)
}
\description{
This function fits two independent XGBoost models for the binary response Y, one based on the treated observations (Trt = 1), and one based on the control observations (Trt = 0)

This function tunes parameters of XGBoost models using a grid search over supplied parameter ranges such as nrounds, eta, subsample, depth. Found parameters are passed in TrainFunc
}
\examples{

# Generating dataset
N = 1000
Trt = rbinom(N,1,0.5)
X = data.frame(X1=rbinom(N,1,0.6), X2=rnorm(N), X3=rnorm(N))
Y = as.numeric( ( 2*X$X1 - 1 + X$X2*Trt + rnorm(N) ) > 0 )
# Fitting models
train2MXGb(Y, Trt, X, opts = list(Trt = list(nrounds = 15, eta = 0.3, subsample = 0.5, depth = 4), Non_Trt = list(nrounds = 10, eta = 0.4, subsample = 0.75, depth = 3)))


# Generating dataset
N = 1000
Trt = rbinom(N,1,0.5)
X = data.frame(X1=rbinom(N,1,0.6), X2=rnorm(N), X3=rnorm(N))
Y = as.numeric( ( 2*X$X1 - 1 + X$X2*Trt + rnorm(N) ) > 0 )
# Fiting model
train2MXGb(Y, Trt, X, opts = list(nrounds = c(10, 15, 20), eta = c(0.3, 0.35), subsample = c(0.5, 0.6, 0.8), depth = c(2, 4, 5)))

}
